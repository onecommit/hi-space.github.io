---
title: Semi-supervised learning
category: AI
tags: ai ğŸ”¥
article_header:
    type: overlay
    theme: dark
    background_color: "#123"
    background_image: false
cover: /assets/images/21-10-01-semi-supervised-learning-semi-supervised-learning.png
---

<!--more-->

# Approach

Semi-supervised learning ì€ ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ í•™ìŠµ ë°©ë²•ì´ë‹¤. ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ë¡œë§Œ í•™ìŠµí•˜ê²Œ ë˜ë©´ overfitting ê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ í¬ë‹¤. ê·¸ë˜ì„œ **ì†ŒëŸ‰ì˜ labeled ë°ì´í„°ì™€ ëŒ€ëŸ‰ì˜ unlabeled ë°ì´í„°ë¥¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¤ê¸° ìœ„í•´ semi-supervised learningì´ ì‚¬ìš©ëœë‹¤. 

![](/assets/images/21-10-01-semi-supervised-learning-semi-supervised-learning.png)

- **Generative Models**: Gaussian Mixture, Deep Generative Model
- **Graph Based**: Label Propagation
- **Self-Training**: Self-Training, Co-Training
- **Consistency Regularization**: Pi-Model, Mean Teacher

ì´ì™€ ê°™ì€ ì ‘ê·¼ë²•ë“¤ì´ ë§ì´ ì‚¬ìš©ë˜ì–´ ì™”ê³  ë”ë¶ˆì–´ ë‹¤ì–‘í•œ regularization ë°©ë²•ë“¤ì´ ì ìš©ë˜ì–´ ì™”ë‹¤. (Weight decay, Mixup, Entropy Minimization)

# 1. Self-Training

ê°€ì¥ ê°„ë‹¨í•œ semi-supervised learning ë°©ë²•ìœ¼ë¡œ, Labeled dataë¡œ ì¶©ë¶„íˆ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ unlabeled dataì— pseudo-labelingì„ í•œë‹¤. ê·¸ë¦¬ê³  labeled data ì™€ pseudo-labeled data ë¥¼ ì„ì–´ì„œ í•™ìŠµì„ í•˜ëŠ” ë°©ì‹ì´ë‹¤. í•™ìŠµ ì´ˆê¸°ì—ëŠ” labeled ë°ì´í„°ë§Œ í•™ìŠµì— ì‚¬ìš©í•˜ë‹¤ê°€ ì ì§„ì ìœ¼ë¡œ pseudo-labeling ë°ì´í„°ë“¤ë„ í•™ìŠµì— ì‚¬ìš©í•œë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-self-training.png)

1. ì¶©ë¶„í•œ Labeled dataë¡œ ëª¨ë¸ì„ í•™ìŠµ
2. 1ë²ˆì„ í†µí•´ í•™ìŠµëœ ëª¨ë¸ë¡œ Unlabeled data ë¥¼ prediction
3. Prediction ê°’ ì¤‘ì—ì„œ confidenceê°€ ë†’ì€ ê²ƒì— Pseudo Labeling
4. Labeled data ì— Pseudo Labeled ë°ì´í„°ë¥¼ í¬í•¨ì‹œí‚¤ê³  1ë²ˆë¶€í„° ë°˜ë³µ

iterationì„ ë°˜ë³µí•  ìˆ˜ë¡ Labeled ë°ì´í„°ê°€ ë”ìš± ëŠ˜ì–´ë‚˜ê³ , í™•ì¥ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ë„ í•¨ê»˜ í–¥ìƒë  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì²˜ìŒ 1ë²ˆ ê³¼ì •ì—ì„œ ì–»ì–´ì§€ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šìœ¼ë©´ Pseudo labeling ì‘ì—…ì—ì„œ error rate ê°€ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ì „ì²´ì ì¸ ëª¨ë¸ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ë„ ìˆë‹¤.

- ê°€ì¥ ê°„ë‹¨í•œ semi-supervised learning
- ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì´ë¼ë„ ì ìš© ê°€ëŠ¥ (wrapper method)
- ì´ˆë°˜ì˜ ì˜ëª»ëœ ì˜ˆì¸¡ì´(early mistakes) ì˜ëª»ëœ ê¸¸ë¡œ ì¸ë„í•  ìˆ˜ ìˆë‹¤.
- ì™„ì „íˆ converge í•œë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤

## Pseudo-label

> [Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks(2013)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.664.3543)

2013ë…„ì— ë°œí‘œëœ Pseudo-Label ë…¼ë¬¸ì´ë‹¤. Labeled ì´ë¯¸ì§€ì™€ Unlabeled ì´ë¯¸ì§€ë¥¼ ë™ì‹œì— í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-2021-10-04-11-59-23.png)

1. Labeled ë°ì´í„°ë¥¼ ì¼ë°˜ì ì¸ Cross-entropy lossì˜ Supervised learning ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
2. Labeled ë°ì´í„°ë¡œ í•™ìŠµëœ ë™ì¼í•œ ëª¨ë¸ì„ í†µí•´ Unlabeled ë°ì´í„°ë¥¼ prediction í•œë‹¤. ì´ ë•Œ, Pseudo-labelë¡œ maximum confidence class ê°€ ì‚¬ìš©ëœë‹¤ (confidenceê°€ ë†’ì€ í´ë˜ìŠ¤ë¥¼ true label ë¡œ ì‚¬ìš©)
3. Model predictionê³¼ unlabeled ë°ì´í„°ì˜ pseudo labelì„ ë¹„êµí•˜ì—¬ Cross-entroy lossê°€ ê³„ì‚°ëœë‹¤.

$$ L = L_{labeled} + \alpha_{t} * L_{unlabeled} $$

ì²˜ìŒ $\alpha_{t}$ ëŠ” 0 ~ 100 step ê¹Œì§€ 0ìœ¼ë¡œ ì„¤ì •í•´ì„œ labeled ë°ì´í„°ë¡œë§Œ ëª¨ë¸ì´ í•™ìŠµë˜ë„ë¡ ì„¤ì •í•˜ê³ , 100 step ì´í›„ë¶€í„°ëŠ” $\alpha_{t}$ ì˜ ê°’ì„ ëŠ˜ë ¤ë‚˜ëŠ” í˜•íƒœë¡œ í•™ìŠµí•œë‹¤. ì´ì™€ ê°™ì´ ì„¤ì •í•˜ë©´ optimizationì´ local minimaì— ë¹ ì§€ëŠ” ê²ƒì„ ì˜ˆë°©í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤ê³  í•œë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-2021-10-04-12-03-33.png)

## Noisy Student

> [Noisy Student: Self training with Noisy Student improves ImageNet classification (CVPR 2020)](https://arxiv.org/pdf/1911.04252.pdf)

Teacher-Student ëª¨ë¸ì„ êµ¬ì„±í•˜ì—¬ ê°ê° training ì‹œí‚¤ëŠ” ë°©ì‹.

1. labeled imageë¡œ teacher modelì„ í•™ìŠµ
2. í•™ìŠµí•œ teacher modelë¥¼ ì‚¬ìš©í•´ ë§ì€ unlabeled imageì— pseudo labelì„ ìƒì„±
   - ë†’ì€ auccuracy ë¡œ labeling í•˜ê¸° ìœ„í•´ Noiseë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
3. labeled imageì™€ pseudo labeled imageë¥¼ ê²°í•©í•˜ê³  noisyë¥¼ ì¶”ê°€í•˜ì—¬ student modelì„ í•™ìŠµ
   - Noisy: dropout, stochastic depth, data augmentation
   - Noisy Studnet modelì€ teacher model ë³´ë‹¤ í¬ë‹¤
4. í•™ìŠµëœ student modelì„ teacherë¡œ ì‚¬ìš©í•˜ì—¬ pseudo labelì„ ë‹¤ì‹œ ìƒì„± (step 2 ë¶€í„° ë°˜ë³µ)

![](/assets/images/21-10-01-semi-supervised-learning-noisy-student.png)

## Meta Pseudo Labels

> [Meta Pseudo Labels CVPR(2021)](https://arxiv.org/pdf/2003.10580.pdf)

Noisy Studnet ì—ì„œëŠ” Teacherì™€ student ëª¨ë¸ì´ ë”°ë¡œ í•™ìŠµë˜ì—ˆë‹¤. Teacher ëª¨ë¸ì´ labeled ë°ì´í„°ë¡œ ì˜ í•™ìŠµë˜ì§€ ì•Šìœ¼ë©´ student ëª¨ë¸ì—ê²Œ knowledge distilation í•  ë•Œ ì˜ëª»ëœ ì§€ì‹ì„ ì „íŒŒí•  ìˆ˜ ìˆë‹¤. Meta Pseudo labelsì€ ê·¸ëŸ¬í•œ ë¬¸ì œì ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Teacher ì™€ student ëª¨ë¸ì„ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-meta-pseudo-labels.png)

Student ê°€ í•™ìŠµë˜ëŠ” ë™ì•ˆ labeled image ì— ëŒ€í•œ studnet ì˜ ì„±ëŠ¥ì´ teacher ì—ê²Œ reward ë¡œ ì „ë‹¬ë˜ê³ , ì´ê²ƒì„ Loss í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ì‚¬ìš©í•œë‹¤. 
- StudentëŠ” teacherì—ì„œ ìƒì„±ëœ pseudo labeled dataë¡œ í•™ìŠµ
- Teacherì€ studentê°€ labeled imageì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ì˜ reward signalë¡œ í•™ìŠµ

# 2. Consistency regularization

![](/assets/images/21-10-01-semi-supervised-learning-2021-10-04-12-57-49.png)

ì´ ë°©ë²•ì€ Unlabeled ë°ì´í„°ì— noiseë¥¼ ì¶”ê°€í•˜ë”ë¼ë„ ê·¸ predictionì€ ìœ ì§€ëœë‹¤ëŠ” ì•„ì´ë””ì–´ë¡œ ì‹œì‘í•œë‹¤. **noiseê°€ ì—†ëŠ” ì›ë³¸ ë°ì´í„°ì™€ noiseê°€ ì£¼ì…ëœ ë°ì´í„°ë¥¼ ë™ì¼í•œ class ë¶„í¬ë¡œ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ**í•˜ëŠ” ê²ƒì´ Consistency regularization ë°©ë²•ì´ë‹¤. noiseëŠ” image augmentation, gaussian noiseì™€ ê°™ì€ input noiseë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³  dropoutì„ ì‚¬ìš©í•´ architecture ìì²´ì— í†µí•©ë  ìˆ˜ë„ ìˆë‹¤.

1. ëª¨ë¸ì„ ì´ìš©í•´ Unlabeled dataì˜ distributionì„ prediction
2. Unlabeled data ì— noise ì¶”ê°€ (data augmentation)
3. 1ë²ˆì˜ prediction ì„ augmented data ì˜ ì •ë‹µ labelë¡œ ì‚¬ìš©í•´ ëª¨ë¸ì„ í•™ìŠµ

- ë°ì´í„°ì— ì•½ê°¼ì˜ augmentationì„ ê°€í•˜ë”ë¼ë„ ë°ì´í„°ì˜ ì •ë‹µ labelì€ ë™ì¼í•˜ë‹¤ëŠ” ì„±ì§ˆì„ ì´ìš©. 
- ìµœê·¼ SSL SOTAì—ì„œ ë§ì´ ì‚¬ìš©ë¨
- ë™ì¼í•œ ì´ë¯¸ì§€ë¥¼ ê°ê¸° ë‹¤ë¥´ê²Œ augmentation í•´ì„œ predictive probability distributionì„ ìœ ì‚¬í•˜ê²Œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤. (lossëŠ” ë‘ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´)

> **Consistency Training**: SSLì˜ ë¶„ì•¼ ì¤‘ í•˜ë‚˜ë¡œ, ì…ë ¥ ë°ì´í„°/í”¼ì²˜ë§µ/ì€ë‹‰ì¸µì— ì•½ê°„ì˜ ë…¸ì´ì¦ˆ(Gaussian/dropout/adversarial)ë¥¼ ì¶”ê°€í–ˆì„ë•Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ë°”ë€Œì§€ ì•Šë„ë¡ ëª¨ë¸ì„ ê·œì œí•˜ë©° í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-consistency-regularization.png)

formula: 

$$ \sum_{b=1}^{\mu B} \parallel p_m(y | \alpha(u_b)) - p_m(y | \alpha (u_b)){\parallel}_2^2 $$


## $\Pi$-model

> [$\Pi$-model: Temporal Ensembling for Semi-Supervised Learning (ICLR 2017)](https://arxiv.org/abs/1610.02242)

Consistency Trainingì˜ ì´ˆì„ì´ ëœ ë…¼ë¬¸ìœ¼ë¡œ, labeled/unlabeled ì´ë¯¸ì§€ ëª¨ë‘ì— augmentationì„ ì·¨í•´ prediction í•˜ëŠ” ë°©ì‹ì´ë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-2021-10-04-13-08-31.png)

1. Labeled ë°ì´í„°ì™€ Unlabeled ë°ì´í„° ìƒê´€ì—†ì´, ë¬´ì‘ìœ„ë¡œ 2ê°œì˜ augmentation ì„ ì·¨í•œë‹¤
2. Dropout ì´ ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ augmentationëœ 2ê°œì˜ ë°ì´í„°ë¥¼ prediction
3. 2ê°œì˜ predictionì˜ ì œê³± ì°¨ì´ë¥¼ consistenccy loss ë¡œ ì‚¬ìš©
4. augmentation ì·¨í•œ ë°ì´í„°ê°€ labeled ë°ì´í„°ë¼ë©´ cross-entropy loss ë„ í•¨ê»˜ ê³„ì‚°
5. Total Loss = cross-entropy loss + $w(t)$ * consistency loss

ì €ìëŠ” Labeled ë°ì´í„°ë§Œ ì‚¬ìš©í•œ SLì—ì„œë„ ì´ ë°©ë²•ì´ Consistencyë¥¼ ì¦ê°€ì‹œì¼œ ëª¨í˜¸í•œ sampleë„ ë” ì˜ êµ¬ë¶„í•œë‹¤ê³  ì£¼ì¥í•œë‹¤.

## Mean Teacher

> [Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results](https://arxiv.org/pdf/1703.01780.pdf)

![](/assets/images/21-10-01-semi-supervised-learning-mean-teacher.png)

Teacher modelê³¼ studnet modelì´ ì„œë¡œ ê°ê°ì˜ ëª¨ë¸ì„ ê°€ì§€ê³  ìˆë‹¤.

## Virtual Adversarial Training

> [Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning](https://arxiv.org/pdf/1704.03976.pdf)

Consistency regularizationì„ ìœ„í•´ adversarial ê°œë…ì„ ì‚¬ìš©í•œë‹¤. input ë°ì´í„°ì— ë‹¨ìˆœí•œ noiseë¥¼ ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, adversarial transformationì„ ì‚¬ìš©í•´ lossì˜ ê°’ì„ ìµœëŒ€í•œ í•´ì¹˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-virtual-adversarial-training.png)

1. ì›ë³¸ ë°ì´í„°ì™€ adversarial ë°ì´í„° ê°„ì˜ KL-diverseê°€ ìµœëŒ€í™” ë˜ë„ë¡ ì´ë¯¸ì§€ì˜ adversarial transformationë¥¼ ìƒì„±
2. Labeled/Unlabeled ë°ì´í„° ì›ë³¸ê³¼ adversarial transformation ë°ì´í„°ì—ì„œ ê°ê° model prediction
3. KL-divergenceëŠ” consistency lossë¡œ ì‚¬ìš©
4. Labeled ì´ë¯¸ì§€ì˜ ê²½ìš° cross-entropy lossë„ ê³„ì‚°
6. Total loss = cross-entropy loss + $\alpha$ * consistency loss

## Unsupervised Data Augmentation

> [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/pdf/1904.12848.pdf)

ë‹¨ìˆœí•œ noise ëŒ€ì‹  ìµœì‹  data augmentation ê¸°ë²•ì¸ AutoAugment ì‚¬ìš©

![](/assets/images/21-10-01-semi-supervised-learning-2021-10-04-13-27-56.png)

1. Unlabeld imageì— AutoAugmentë¥¼ ì‚¬ìš©í•´ augmentation ì´ë¯¸ì§€ë¥¼ ìƒì„±
2. ë™ì¼í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì›ë³¸ ì´ë¯¸ì§€ì™€ augmentation ì´ë¯¸ì§€ë¥¼ prediction
3. ë‘ ê°œì˜ predictionì˜ KL-divergenceëŠ” consistency lossë¡œ ì‚¬ìš©
4. Labeled imageëŠ” consistenccy lossë¥¼ ê³„ì‚°í•˜ì§€ ì•Šê³ , cross-entropy loss ë§Œ ê³„ì‚°
5. Total loss = cross-entropy loss + $\alpha$ * consistency loss

ì´ë¯¸ì§€ ë¿ë§Œ ì•„ë‹ˆë¼ í…ìŠ¤íŠ¸ì—ë„ ì ìš©í•  ìˆ˜ ìˆëŠë°, 20ê°œì˜ labeled ë°ì´í„°ë§Œìœ¼ë¡œ fully supervisedë¥¼ ë›°ì–´ë„˜ëŠ” ì„±ëŠ¥ì„ ëƒˆë‹¤.

# 3. Hybrid

## MixMatch

Entropy minimization, Label consistency regularization, mixupì„ ëª¨ë‘ ì ìš©í•œ ë°©ë²•

> [MixMatch: A Holistic Approach to Semi-Supervised Learning](https://arxiv.org/pdf/1905.02249.pdf)

![](/assets/images/21-10-01-semi-supervised-learning-mix-match-temp-sharpen.png)
![](/assets/images/21-10-01-semi-supervised-learning-mix-match.png)

1. Unlabeld ë°ì´í„°ì— ëŒ€í•´ Kê°œì˜ augmentation ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , predictoin í‰ê· ì„ êµ¬í•´ temperature sharpeningì„ í†µí•´ sharpen í•˜ì—¬ pseudo-labelingìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ (Entropy Minimization)
2. Augmentation ëœ labeld, unlabeld ë°ì´í„°ë¥¼ ëª¨ë‘ ê²°í•©í•´ Mixup í•œë‹¤ 
3. SL lossëŠ” cross-entropy loss, unsupervised loss ëŠ” ëª¨ë¸ ì¶œë ¥ê°’ì˜ ì°¨ì´(MSE)ê°€ ëœë‹¤ (consistency regularization)

## ReMixMatch

- MixMatch í›„ì† ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, Consistency Trainingê³¼ Mixupì„ ì‚¬ìš©í•œ ë°©ë²•. 
- **Distribution Alignment**: Unlabeled dataì˜ predictive distributionì„ labeled dataì˜ ì‹¤ì œ ë¶„í¬ì— ã…ìì¶° ì¡°ì •
- **Augmentation Anchoring**: Augmentation í•  ë•Œ Weak augmentationì´ ì ìš©ëœ ë°ì´í„° ì˜ˆì¸¡ ë¶„í¬ë¥¼ strong augmentationì´ ì ìš©ëœ ë°ì´í„°ì˜ targetìœ¼ë¡œ ì´ìš©

## FixMatch

> [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://arxiv.org/pdf/2001.07685.pdf)

Pseudo-labelingê³¼ consistency regularizationì„ ê²°í•©í•œ ë°©ë²•. weak augmentationê³¼ strong augmentation(RandAugment, CTAugment)ì— pseudo-labeling ë°©ë²•ì„ í•¨ê»˜ ë„ì…

![](/assets/images/21-10-01-semi-supervised-learning-fixmatch.png)

1. Labled ë°ì´í„°ë¥¼ í†µí•´ SL í•™ìŠµ (cross-entropy loss)
2. Unlabeled ë°ì´í„°ë¥¼ Weakly-augmentation í•˜ê³  ëª¨ë¸ì„ í†µí•´ prediction ê°’ì„ ì–»ì–´ë‚¸ë‹¤.
3. Class probabilityì—ì„œ threshold ê°’ì„ ë„˜ëŠ” í´ë˜ìŠ¤ë¥¼ one-hot pseudo-labelì„ ì·¨í•œë‹¤. (í•´ë‹¹ í´ë˜ìŠ¤ì˜ í™•ë¥ ì„ 1ë¡œ)
4. ê°™ì€ ì´ë¯¸ì§€ì— ëŒ€í•´ Strong-augmentation í•œë‹¤
5. Strong augmentation ì´ë¯¸ì§€ë¥¼ inputìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì–»ì–´ì§„ class probability ì™€(Cross-entropy loss), weaky augmentationì—ì„œ ì–»ì–´ì§„ pseudo label ê²°ê³¼ì™€ í•¨ê»˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤. 

# Regularization

## Entropy Minimization

prediction probabilityì˜ confidenceë¥¼ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. ì£¼ë¡œ softmax temperatureë¥¼ ì´ìš©

ëª¨ë¸ì„ ì¢€ ë” confident í•˜ê²Œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. lossì— entropy minimization term ì„ ì¶”ê°€í•˜ì—¬ í•™ìŠµí•œë‹¤.

## Mixup, Cutout, CutMix

ì´ë¯¸ì§€ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Regularization ë°©ë²•ìœ¼ë¡œ, ê°„ë‹¨í•˜ì§€ë§Œ ë†’ì€ íš¨ê³¼ë¥¼ ë³´ì´ê³  ìˆë‹¤.

![](/assets/images/21-10-01-semi-supervised-learning-cutmix.png)

# Evaluation

Semi-supervised learning ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´, ë³´í†µ Labled ë°ì´í„°ë¥¼ ì ê²Œ ì‚¬ìš©í•˜ê³  Unlabeld ë°ì´í„°ë¥¼ ë§ì´ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸í•œë‹¤.

# Referecne

- [Semi-supervised learning ë°©ë²•ë¡  ì†Œê°œ](https://blog.est.ai/2020/11/ssl/)
- [Semi-supervised learning ì •ë¦¬](https://jiwunghyun.medium.com/semi-supervised-learning-%EC%A0%95%EB%A6%AC-a7ed58a8f023)
- [FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence](https://2-chae.github.io/category/2.papers/29)
- [FixMatch : Simplifying Semi-Supervised Learning with Consistency and Confidence](https://cool24151.tistory.com/81)
- [Consistency Regularization](https://seewoo5.tistory.com/8)
- [semi-supervised learning](https://amitness.com/2020/07/semi-supervised-learning/)
- [Meta Pseudo Labels](https://medium.com/@nainaakash012/meta-pseudo-labels-6480acb1b68)
- [[Paper Review] ReMixMatch & FixMatch : Consistency-based Semi-supervised Learning Methods](http://dsba.korea.ac.kr/seminar/?mod=document&uid=248)