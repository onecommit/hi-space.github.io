---
title: Contrastive Learning
category: AI
tags: ai ğŸ”¥
---

ì—¬ëŸ¬ ì´ë¯¸ì§€ë“¤ ì¤‘ ë¹„ìŠ·í•œ ì´ë¯¸ì§€(positive pair)ë¥¼ ì„œë¡œ ê°€ê¹ê²Œ í•˜ë©´ì„œ, ë™ì‹œì— ì´ ë‘˜ì„ ë¹„ìŠ·í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€(negative pair)ì™€ ë©€ë¦¬ ë–¨ì–´ì§€ê²Œ í•˜ëŠ” ê²ƒ

<!--more-->

# Introduction

human supervision ì—†ì´ un-supervisionìœ¼ë¡œë§Œ visual representation í•™ìŠµí•˜ê¸° ìœ„í•´ ë§ì€ approach ê°€ ìˆì—ˆë‹¤.

### Generative approaches

- image translation í•˜ë©´ì„œ (unet  ê°™ì€..) pixel levelë¡œ generation í•˜ëŠ” ë°©ì‹.
- pixel level ì—°ì‚°ì´ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ë§¤ìš° ë¹„ì‹¸ê³ , representation learning í•˜ëŠ” ë°ì— ê¼­ í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤.
- ì‘ìš©í•˜ì—¬ ì—°êµ¬ê°€ ë§ì´ ë‚˜ì˜¤ê¸´ í•˜ì§€ë§Œ ì„±ëŠ¥ì´ ì•„ì£¼ ì¢‹ì§€ ì•Šë‹¤
- pixel level generation is **computationally expensive** and **may not be necessary** for representation learning

### Discriminative Approches

- pretext task ë¥¼ ì´ìš©í•˜ì—¬ representation learning ì„ í•œë‹¤.
- generalization ì„±ëŠ¥ì— í•œê³„ê°€ ìˆì„ ìˆ˜ ìˆë‹¤.
- learn representations using objective function like supervised learning but **pretext tasks have relied on somewhat ad-hoc heuristics**, which **limits the generality of learn representations**

### Contrastive Learning

![](/assets/images/21-09-04-constrastive-learning-2021-09-04-19-04-39.png)

- Nê°œì˜ input data ê°€ ë“¤ì–´ì™”ì„ ë•Œ ë°ì´í„° ê°„ì— ìœ ì‚¬í•œì§€, ìœ ì‚¬í•˜ì§€ ì•Šì€ì§€ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹

---

# Concept

Contrastive Learningì€ Positive pairì™€ Negative Pairë¡œ êµ¬ì„±ëœë‹¤. Positive pair ë¼ë¦¬ëŠ” ê±°ë¦¬ë¥¼ ì¢íˆê³ , Negative pair ë¼ë¦¬ëŠ” ê±°ë¦¬ë¥¼ ë©€ë¦¬ ë„ì›Œë†“ëŠ” ê²ƒì´ í•™ìŠµ ì›ë¦¬ì´ë‹¤.

- ê°™ì€ imageì— ì„œë¡œ ë‹¤ë¥¸ augmentationì„ ê°€í•œ ë’¤, ë‘ positive pairì˜ feature representationì€ ê±°ë¦¬ê°€ ê°€ê¹Œì›Œì§€ë„ë¡ í•™ìŠµì„ í•˜ê³ 
- ë‹¤ë¥¸ imageì— ì„œë¡œ ë‹¤ë¥¸ augmentationì„ ê°€í•œ ë’¤, ë‘ negative pairì˜ feature representation ì€ ê±°ë¦¬ê°€ ë©€ì–´ì§€ë„ë¡ í•™ìŠµì„ í•œë‹¤.

![](/assets/images/21-09-04-self-supervised-learning-2021-09-04-16-01-14.png)

ìœ„ì˜ ì˜ˆì‹œë¥¼ ë´¤ì„ ë•Œ, ê°•ì•„ì§€ì™€ ì˜ìë¥¼ ê°ê° augmentation ì·¨í•œ í›„,

ë™ì¼í•œ imageë¡œ augmentation ëœ ê²ƒ ë¼ë¦¬ëŠ” ê°ê° positive pairë¡œ í•™ìŠµì‹œí‚¤ê³ , 

ë‹¤ë¥¸ imageì˜ augmentationëœ ê²ƒê³¼ëŠ” negative pairë¡œ í•™ìŠµì‹œí‚¨ë‹¤.

---

# Related Paper

## SimCLR

![](https://camo.githubusercontent.com/5ab5e0c019cdd8129b4450539231f34dc028c0cd64ba5d50db510d1ba2184160/68747470733a2f2f312e62702e626c6f6773706f742e636f6d2f2d2d764834504b704539596f2f586f3461324259657276492f414141414141414146704d2f766146447750584f79416f6b4143385868383532447a4f67457332324e68625877434c63424741735948512f73313630302f696d616765342e676966)

- í•˜ë‚˜ì˜ ì´ë¯¸ì§€ë¡œë¶€í„° transformation í•´ì„œ ë³€í™˜ëœ ë·°ì˜ ì´ë¯¸ì§€ë“¤ì„ ë½‘ì•„ë‚¸ë‹¤. ë³€í™˜ëœ ë·° ê°„ì˜ ì¼ì¹˜ë¥¼ ìµœëŒ€í™”í•˜ê³  ë‹¤ë¥¸ ì´ë¯¸ì§€ì˜ ë³€í™˜ëœ ë·° ê°„ì˜ ì¼ì¹˜ë¥¼ ìµœì†Œí•˜ë©° í•™ìŠµí•´ê°„ë‹¤.
- ëŒ€ì‘í•˜ëŠ” ë·°: attract
- ëŒ€ì‘í•˜ì§€ ì•ŠëŠ” ë·°: repel
- ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ sampleì„ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•˜ê³  ë‘ë²ˆì˜ transformationì„ í†µí•´ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤
    - transformation ì¢…ë¥˜: random cropping, random color distortion, gaussian blur, sobel filtering, etc.
    - transform ì‹œ ë™ì¼í•œ ì´ë¯¸ì§€ì˜ ì¼ê´€ëœ representationì„ ì¥ë ¤í•œë‹¤

> **My Insight**: synthetic dataì˜ íŠ¹ì • classì™€ real dataì˜ ë™ì¼í•œ class ê°„ì—ëŠ” attract í•˜ë‹¤ê³  í•˜ê³ , ë‹¤ë¥¸ classì— ëŒ€í•´ repel í•˜ë‹¤ê³  í•™ìŠµí•˜ë©´ gapì„ ì¤„ì¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

- batch size ê°€ ì—„ì²­ í¬ë‹¤ (8192)
- í›„ì† ë…¼ë¬¸: Big Self-Supervised Models are Strong Semi-Supervised Learners

### Experiments

#### **Augmentation**

- Random crop and resize (with random flip)
- Color distortions
- Gaussian blur

#### Encoder

- Representation ì„ í•˜ê¸° ìœ„í•´ ResNet50 ì‚¬ìš© â†’ 2048 dimension

#### Projection

- similarityë¥¼ ê³„ì‚°í•˜ê¸° ì¢‹ì€ domain ìœ¼ë¡œ ì˜®ê²¨ì£¼ê¸° ìœ„í•´ 2-layer MLP ì‚¬ìš© â†’ 128 dimensional latent space

#### No negative Sampling

- Batch ì‚¬ì´ì¦ˆê°€ ì¶©ë¶„íˆ í¬ë©´ ë³¸ì¸ ì™¸ì˜ ê°’ë“¤ì€ negative sampleì´ ë˜ê¸° ë•Œë¬¸ì— ë”°ë¡œ negative sampling ì‘ì—…ì„ í•´ì£¼ì§€ ì•ŠìŒ

#### Similarity Metric

- Cosine similarity function

#### Loss

- Normalized temperature-scaled cross entropy(NT-Xent) loss

#### Evaluation Metric

- linear evaluation protocol
- random init í›„ freeze ì‹œí‚¨ feature extractorì— 1ê°œì˜ linear layerë¥¼ ë¶™ì—¬ì„œ ImageNet ë°ì´í„°ì…‹ë¡œ í•™ìŠµì‹œí‚¨ ë’¤ ì •í™•ë„ë¥¼ ì¸¡ì •
- self supervised learning ìœ¼ë¡œ í•™ìŠµëœ pretrain ì„ freeze í•´ì„œ ë”ì´ìƒ í•™ìŠµì´ ì•ˆë˜ê²Œ ë§‰ê³ , linear layer í•˜ë‚˜ë§Œ ì¶”ê°€í•´ì„œ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë˜ë‚˜ ì¸¡ì •

---

## BYOL (Bootstrap Your Own Latent)

[TBD]

---

# Reference

- [https://github.com/google-research/simclr](https://github.com/google-research/simclr)
- [https://brunch.co.kr/@synabreu/76](https://brunch.co.kr/@synabreu/76)
- [PR-231: A Simple Framework for Contrastive Learning of Visual Representations](https://www.youtube.com/watch?v=FWhM3juUM6s)
- [The Illustrated SimCLR Framework](https://amitness.com/2020/03/illustrated-simclr/)
- [SimCLRì„ ì´ìš©í•œ í–¥ìƒëœ ìê¸°ì£¼ë„ ë° ë°˜ì£¼ë„ í•™ìŠµ](https://brunch.co.kr/@synabreu/76)
- [ê³ ë ¤ëŒ€í•™êµ DMQA ì—°êµ¬ì‹¤](http://dmqm.korea.ac.kr/activity/seminar/284)
