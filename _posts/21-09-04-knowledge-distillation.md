---
title: Knowledge Distillation
category: AI
tags: ai paper ğŸ”¥
---

Ensemble ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” generalization ëŠ¥ë ¥ì„ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ ê·œëª¨ì˜ networkê°€ ë°›ì„ ìˆ˜ ìˆë‹¤ë©´ ì¢‹ì§€ ì•Šì„ê¹Œ? 

<!--more-->

Knowledge DistillationëŠ” Distilling the Knowlege in a Neural Network (NIPS 2014 Workshop) ì—ì„œ ì²˜ìŒ ì œì•ˆëœ ê°œë…ì´ë‹¤. ì´ ë…¼ë¬¸ì€ ê·¸ ë°©ë²•ì— ëŒ€í•´ ì œì•ˆí•˜ê³  ìˆë‹¤.

## Knowledge Distilation (ì§€ì‹ ì¦ë¥˜)

- í° ëª¨ë¸ (Teacher Network)ë¡œë¶€í„° ì¦ë¥˜í•œ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸ (Student Network)ë¡œ  transfer í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •
- Model deployment ì¸¡ë©´ì—ì„œ í•„ìš”í•œ ê°œë…ì´ë‹¤. ì‹¤ì œ ëª¨ë¸ì„ ì–´ë–¤ í•˜ë“œì›¨ì–´ì— ë°°í¬í•  ë•Œ, ì²˜ìŒ í•™ìŠµì‹œí‚¨ í•˜ë“œì›¨ì–´ì— ë¹„í•´ íƒ€ê²Ÿ í•˜ë“œì›¨ì–´ì˜ ì„±ëŠ¥ì€ ì¢‹ì§€ ì•Šì„ê±°ë‹¤. ê·¸ë˜ì„œ ëª¨ë¸ì„ ì¢€ ë” ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤
- Teacher Networkê°€ í•™ìŠµí•œ generalization ëŠ¥ë ¥ì„ Student Networkì— transfer í•´ì£¼ëŠ” ê²ƒì´ë‹¤

### Teacher Network

- Cumbersome model (ex, ensemble, a large generalized model)
- Pros : ì„±ëŠ¥ì´ ì¢‹ë‹¤
- Cons : computationally ë¹„ì‹¸ë‹¤
- ì œì•½ì´ ìˆëŠ” í™˜ê²½ì— ë°°í¬í•˜ê¸° ì–´ë µë‹¤

### Student Network

- Small model
- ë°°í¬ ì‹œì— ì í•©í•˜ë‹¤
- Pros : inferenceê°€ ë¹ ë¥´ë‹¤
- Cons : Teacher network ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤

## Soft Label

Knowledge Distiliationì„ ìœ„í•´ Soft Label ì„ ì‚¬ìš©í•œë‹¤. 

- ë³´í†µ Classification í•™ìŠµì„ í•  ë•Œ one-hot encodingì´ ëœ Hard Labelì„ ì´ìš©í•´ í•™ìŠµí•œë‹¤. ì—¬ê¸°ì„œ hardë¥¼ discrete ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. [0, 1, 0, 0]
- Soft labelì€ discreteí•œ ê°’ì´ ì•„ë‹Œ í™•ë¥ ê°’ì„ labelë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. [.05, .3, .2, .005]