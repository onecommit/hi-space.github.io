---
title: Knowledge Distillation
category: AI
tags: ai paper ğŸ”¥
---

Knowledge DistillationëŠ” Distilling the Knowlege in a Neural Network (NIPS 2014 Workshop) ì—ì„œ ì²˜ìŒ ì œì•ˆëœ ê°œë…ì´ë‹¤. ê¸°ì¡´ì˜ ì•™ì‚¬ë¸”ì˜ ê±°ëŒ€í•˜ê³  í° ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ëœ í™•ë¥ ì„ Pseudo labelë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. 

<!--more-->

## Knowledge Distilation (ì§€ì‹ ì¦ë¥˜)

- í° ëª¨ë¸ (Teacher Network)ë¡œë¶€í„° ì¦ë¥˜í•œ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸ (Student Network)ë¡œ  transfer í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •
- Model deployment ì¸¡ë©´ì—ì„œ í•„ìš”í•œ ê°œë…ì´ë‹¤. ì‹¤ì œ ëª¨ë¸ì„ ì–´ë–¤ í•˜ë“œì›¨ì–´ì— ë°°í¬í•  ë•Œ, ì²˜ìŒ í•™ìŠµì‹œí‚¨ í•˜ë“œì›¨ì–´ì— ë¹„í•´ íƒ€ê²Ÿ í•˜ë“œì›¨ì–´ì˜ ì„±ëŠ¥ì€ ì¢‹ì§€ ì•Šì„ê±°ë‹¤. ê·¸ë˜ì„œ ëª¨ë¸ì„ ì¢€ ë” ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤
- Teacher Networkê°€ í•™ìŠµí•œ generalization ëŠ¥ë ¥ì„ Student Networkì— transfer í•´ì£¼ëŠ” ê²ƒì´ë‹¤

### Teacher Network

- Cumbersome model (ex, ensemble, a large generalized model)
- Pros : ì„±ëŠ¥ì´ ì¢‹ë‹¤
- Cons : computationally ë¹„ì‹¸ë‹¤
- ì œì•½ì´ ìˆëŠ” í™˜ê²½ì— ë°°í¬í•˜ê¸° ì–´ë µë‹¤

### Student Network

- Small model
- ë°°í¬ ì‹œì— ì í•©í•˜ë‹¤
- Pros : inferenceê°€ ë¹ ë¥´ë‹¤
- Cons : Teacher network ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤

### Soft Label

Knowledge Distiliationì„ ìœ„í•´ Soft Label ì„ ì‚¬ìš©í•œë‹¤. 

- ë³´í†µ Classification í•™ìŠµì„ í•  ë•Œ one-hot encodingì´ ëœ Hard Labelì„ ì´ìš©í•´ í•™ìŠµí•œë‹¤. ì—¬ê¸°ì„œ hardë¥¼ discrete ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. [0, 1, 0, 0]
- Soft labelì€ discreteí•œ ê°’ì´ ì•„ë‹Œ í™•ë¥ ê°’ì„ labelë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. [.05, .3, .2, .005]

# Reference

- [Knowledge Distillation ë¦¬ë·°](https://ezobear.github.io/model%20compression/2020/01/02/KD-post.html)